{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ac3fe2b3-5a97-4912-91dc-b4efce5befa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import glob\n",
    "\n",
    "import netCDF4\n",
    "\n",
    "import xarray as xr\n",
    "\n",
    "%load_ext memory_profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b5878c85-2986-4616-a34f-db5c64b116d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/g/data/tm70/ds0092/projects/um_output_memory\n"
     ]
    }
   ],
   "source": [
    "%cd /g/data/tm70/ds0092/projects/um_output_memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3af7467f-3070-4113-a023-7c4086df9314",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = \"./cj877a.pm000101_mon.1x1.nc4\" # File is 1.5 MB\n",
    "variable = \"fld_s03i807\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6221e23-ef4a-4890-aabc-1bf5669da6cc",
   "metadata": {},
   "source": [
    "# Opening a single 1.5MB files uses ~20MB of memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "feeabfa2-f5da-4581-89e7-00ef9fd621e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "peak memory: 142.83 MiB, increment: 19.13 MiB\n",
      "CPU times: user 154 ms, sys: 37.5 ms, total: 192 ms\n",
      "Wall time: 303 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%%memit\n",
    "\n",
    "f = netCDF4.Dataset(file)\n",
    "var = f[variable]\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e77d270-0cb3-4663-901d-82a1da78ef64",
   "metadata": {},
   "source": [
    "# Memory grows when opening/concantenating many files with xarray\n",
    "\n",
    "E.g. here we duplicate the file 500 times (750 MB of data in total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f2c31fcd-8bce-46c3-b877-948d46e6582d",
   "metadata": {},
   "outputs": [],
   "source": [
    "copies = []\n",
    "for idx in range(500):\n",
    "    copy = os.path.join(\n",
    "        os.path.dirname(file), f\"copy.{str(idx).zfill(3)}.nc4\"\n",
    "    )\n",
    "    shutil.copyfile(file, copy)\n",
    "    copies.append(copy)\n",
    "    \n",
    "files = glob.glob(\"./copy.*.nc4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "234a08af-87bc-40a3-a590-70d15580b951",
   "metadata": {},
   "source": [
    "### Open 100 files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9be29630-1e51-4c99-9c64-31eaa17e51c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "peak memory: 2358.50 MiB, increment: 2231.26 MiB\n",
      "CPU times: user 27.7 s, sys: 3.02 s, total: 30.7 s\n",
      "Wall time: 36.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%%memit\n",
    "\n",
    "ds = xr.open_mfdataset(\n",
    "    files[:100], \n",
    "    engine=\"netcdf4\",\n",
    "    combine=\"nested\",\n",
    "    concat_dim=\"time\",\n",
    "    decode_cf=False,\n",
    "    decode_coords=False,\n",
    "    decode_times=False,\n",
    "    preprocess=lambda ds: ds[variable]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d57a6fe-d5fe-429e-8a45-a9a7c5fb46db",
   "metadata": {},
   "source": [
    "### Open all 500 files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "01347ff9-6875-4363-82b7-e172c52420a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "peak memory: 3793.55 MiB, increment: 3666.23 MiB\n",
      "CPU times: user 2min 4s, sys: 4.74 s, total: 2min 8s\n",
      "Wall time: 2min 11s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%%memit\n",
    "\n",
    "ds = xr.open_mfdataset(\n",
    "    files, \n",
    "    engine=\"netcdf4\",\n",
    "    combine=\"nested\",\n",
    "    concat_dim=\"time\",\n",
    "    decode_cf=False,\n",
    "    decode_coords=False,\n",
    "    decode_times=False,\n",
    "    preprocess=lambda ds: ds[variable]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad1f77c8-86db-4582-bc3c-69e52d124e7f",
   "metadata": {},
   "source": [
    "### Opening with `h5netcdf` uses less memory but takes a ridiculously long time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "903a6949-ef14-4aa3-b2b9-e72dd3e5d5a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "peak memory: 872.97 MiB, increment: 745.65 MiB\n",
      "CPU times: user 37min 26s, sys: 9.93 s, total: 37min 36s\n",
      "Wall time: 38min 33s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%%memit\n",
    "\n",
    "ds = xr.open_mfdataset(\n",
    "    files[:100], \n",
    "    engine=\"h5netcdf\",\n",
    "    combine=\"nested\",\n",
    "    concat_dim=\"time\",\n",
    "    decode_cf=False,\n",
    "    decode_coords=False,\n",
    "    decode_times=False,\n",
    "    preprocess=lambda ds: ds[variable]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80eeff8e-ef30-440f-9ba1-71e4899d16a8",
   "metadata": {},
   "source": [
    "### Extract variable array, dimensions and attrs explicitly from netCDF4 Dataset\n",
    "\n",
    "Explicitly closing the file here after extracting what we want obviously reduces memory. However, usually we want to operate lazily with xarray meaning the file presumably remains open. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a1bd542c-1b45-4786-9167-bc46b1103aee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "peak memory: 240.80 MiB, increment: 113.45 MiB\n",
      "CPU times: user 44.4 s, sys: 1.04 s, total: 45.4 s\n",
      "Wall time: 46.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%%memit\n",
    "\n",
    "def _open_variable(file, variable):\n",
    "    \"\"\"Pull out variable/dimensions/attributes and pack into xarray DataArray\"\"\"\n",
    "    f = netCDF4.Dataset(file)\n",
    "    data = f[variable][:].data\n",
    "    coords = {}\n",
    "    for dim in f[variable].dimensions:\n",
    "        coords[dim] = f[dim][:].data\n",
    "    attrs = {}\n",
    "    for attr in f[variable].ncattrs():\n",
    "        attrs[attr] = f[variable].getncattr(attr)\n",
    "    f.close()\n",
    "    return xr.DataArray(data, coords, attrs=attrs)\n",
    "\n",
    "ds = []\n",
    "for file in files:\n",
    "    ds.append(_open_variable(file, variable))\n",
    "    \n",
    "ds = xr.concat(ds, dim=\"time\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebf30fa7-04d3-45c2-b0b1-72d93f0da658",
   "metadata": {},
   "source": [
    "# What about netCDF4 MFDataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4c78fa2f-c6fe-40ae-a914-26554f8d7ddf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "peak memory: 9067.20 MiB, increment: 8939.89 MiB\n",
      "CPU times: user 53.3 s, sys: 4.85 s, total: 58.2 s\n",
      "Wall time: 58.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%%memit\n",
    "\n",
    "f = netCDF4.MFDataset(files)\n",
    "var = f[variable]\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "27073568-822f-4601-8baf-520a100bc23c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for copy in copies:\n",
    "    os.remove(copy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4227a48b-7ffb-4d3c-9dbd-9f8ba42acdb3",
   "metadata": {},
   "source": [
    "# What have I tried?\n",
    "\n",
    " - convert to NETCDF3 - this fixes the issue. However, both NETCDF4 and NETCDF4_CLASSIC show the same behaviour\n",
    " - these files have `filling off`. I've recreated the data with `filling on` - no effect\n",
    " - played with chunking of variables in NETCDF4 files - no effect\n",
    " \n",
    "# Things to note\n",
    "\n",
    " - the memory footprint is essentially the same for the reduced-size files here as for the full-size files. The reduced-size files include only one spatial grid point, whereas the full size files include 27,648. That is, it's almost like it's the metadata that is responsible for the large memory footprint.\n",
    " - This file contains 250 variables. I've never worked with NetCDF files containing this many variables - perhaps the problem could be related to this??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa68ac94-c866-4789-b6d3-9671797df12a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:analysis3-unstable]",
   "language": "python",
   "name": "conda-env-analysis3-unstable-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
